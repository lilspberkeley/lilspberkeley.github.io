<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>CS 180 Project 4a</title>
</head>

<body>
  <!-- Title -->
  <div class="navbar clear nav-top">
      <div class="row" style="text-align: center;">
          <h1>CS 180 Project 4a: Colorizing the Prokudin-Gorskii photo collection</h1>
          <b>By Shreyas Prasanna</b>
      </div>
  </div>
  <p>The goal of this assignment is to get our hands dirty in different aspects of image warping with a “cool” application -- image mosaicing. We will take two or more photographs and create an image mosaic by registering, projective warping, resampling, and compositing them. Along the way, we will learn how to compute homographies, and how to use them to warp images.</p>

  <h2>Shoot the Pictures</h2>
  <p>I took a couple of picture sets (2 or 3 imagees) for a few indoor and outdoor scenes that follow the below "rules":</p>
  <ul>
        <li><strong>Projective Transformation:</strong> The images should appear to be taken from a single viewpoint, but with a slight rotation or shift between them. This is the essence of projective transformation, where the perspective changes slightly but objects in the scene remain recognizable.</li>
        
        <li><strong>Sufficient Overlap:</strong> Each photo should overlap with the adjacent ones by 40% to 70%. This is crucial for stitching the images together later, as you’ll need common points or features in both images to compute the homography.</li>
        
        <li><strong>Minimal Distortion:</strong> If the images don’t have significant lens distortion (like fisheye or extreme barrel distortion), then they should be fine for your project. If there is distortion, it can complicate the image warping and blending process.</li>
        
        <li><strong>Consistent Lighting and Scene Stability:</strong> If the photos were taken at the same time or under similar lighting, and there’s no significant movement in the scene (e.g., trees swaying too much, water flowing rapidly), then they will work well for image mosaicing.</li>
    </ul>

  <h2>Recover Homographies</h2>
  <h3>Matrix A Construction</h3>
    <p>
        For each correspondence (x<sub>1</sub>, y<sub>1</sub>) &#8596; (x<sub>2</sub>, y<sub>2</sub>), 
        two rows are added to the matrix <em>A</em>, forming the system:
    </p>
    <pre>
    [
        -x<sub>1</sub>, -y<sub>1</sub>, -1,  0,  0,  0,  x<sub>1</sub>x<sub>2</sub>, y<sub>1</sub>x<sub>2</sub>, x<sub>2</sub>
         0,  0,  0, -x<sub>1</sub>, -y<sub>1</sub>, -1,  x<sub>1</sub>y<sub>2</sub>, y<sub>1</sub>y<sub>2</sub>, y<sub>2</sub>
    ]
    </pre>

    <h3>Solving the System</h3>
    <p>
        We use <strong>Singular Value Decomposition (SVD)</strong> to solve the homogeneous system <em>Ah = 0</em>. 
        The solution vector <em>h</em> (which gives us the homography matrix <em>H</em>) is found from the last row of <em>V<sup>T</sup></em>.
    </p>

    <h3>Normalizing the Homography Matrix</h3>
    <p>
        After computing the homography matrix <em>H</em>, it is normalized by dividing by the bottom-right value <em>H[2,2]</em> 
        to ensure that the scaling factor is 1.
    </p>

  <h2>Warp the Images</h2>
  <h3>Corner Transformation</h3>
    <p>
        The corners of the input image are transformed using the homography matrix <em>H</em> to predict the bounding box of the warped image.
    </p>

    <h3>Meshgrid for Output Coordinates</h3>
    <p>
        We create a grid of output coordinates that span the predicted bounding box. These are the coordinates where we want to compute pixel values for the warped image.
    </p>

    <h3>Inverse Warp</h3>
    <p>
        We apply the inverse homography <em>H<sup>-1</sup></em> to map each pixel in the output image back to a coordinate in the input image.
    </p>

    <h3>Interpolation</h3>
    <p>
        We use <code>scipy.interpolate.griddata</code> to interpolate pixel values at the computed coordinates. This avoids aliasing by smoothly sampling the input image.
    </p>

    <h3>Alpha Mask</h3>
    <p>
        Pixels that are outside the valid bounds of the input image are marked using an alpha mask or set to zero.
    </p>

  <h2>Image Rectification</h2>
  <p>For the process of image rectification, I took a couple of images of rectangular objects, such as a laptop and TV. After which I took the following steps:</p>
  <h3>Steps for Image Rectification</h3>

    <h3>Define Point Correspondences</h3>
    <ul>
        <li>You need to identify four points in your input image (e.g., the corners of a rectangular object like a painting or monitor).</li>
        <li>These points will be stored in <code>im1_pts</code>, representing their coordinates in the image.</li>
        <li>For <code>im2_pts</code>, you define the desired position of these points, typically corresponding to the four corners of a perfect rectangle (e.g., [0, 0], [1, 0], [1, 1], [0, 1]).</li>
    </ul>

    <h3>Compute Homography</h3>
    <ul>
        <li>Using the <code>computeH</code> function that you already developed, compute the homography matrix that maps the points in <code>im1_pts</code> to <code>im2_pts</code>.</li>
    </ul>

    <h3>Warp the Image</h3>
    <ul>
        <li>Use the <code>warpImage</code> function to apply the computed homography and warp the input image so that the rectangular object is rectified.</li>
    </ul>

    <h3>Visualize the Result</h3>
    <ul>
        <li>Display the original and rectified images to verify that the transformation worked as expected.</li>
    </ul>
  <h3>Below are the results of image rectification</h3>
  <h3 id="single-scale-alignment-results">Result-1</h3>
                <div class="gallery">
                    <img src="results/laptop.jpg" />
                    <img src="results/rectified_laptop.jpg" />
                </div>
  <h3 id="single-scale-alignment-results">Result-2</h3>
                <div class="gallery">
                    <img src="results/monument.jpg" />
                    <img src="results/monument_rectified.jpg" />
                </div>

  <h2>Blend the images into a mosaic</h2>
  <p>The approach to create a seamless mosaic between two images involves the following steps:</p>

<ol>
  <li><strong>Identify Corresponding Points</strong>: Manually select common points in both images, such as corners or edges of objects, which will be used to align the images.</li>
  
  <li><strong>Calculate Homography Matrix</strong>: Using the corresponding points, a homography matrix is computed to map points from the left image to the right (root) image, transforming the perspective.</li>

  <li><strong>Warp the Left Image</strong>: The left image is warped based on the homography matrix so that it aligns with the root image, making their perspectives match.</li>

  <li><strong>Blend the Images</strong>: A blending technique (alpha blending) is applied in the overlapping regions of the two images to ensure a smooth transition without visible seams.</li>

  <li><strong>Construct the Mosaic</strong>: The non-overlapping regions from the left image are combined with the rest of the root image, and the result is a seamless mosaic that includes both images.</li>
</ol>

<h3>Key Concepts</h3>
<ul>
  <li><strong>Homography</strong>: Transforms the perspective of one image to match another.</li>
  <li><strong>Warping</strong>: Adjusts the left image to align with the root image.</li>
  <li><strong>Blending</strong>: Ensures smooth transitions between overlapping parts of the images.</li>
</ul>

<p>This method results in a combined mosaic that naturally transitions from one image to the other, with aligned perspectives and blended overlapping areas.</p>


<p>Below are the results</p>
  <h3 id="single-scale-alignment-results">Hall Results</h3>
                <div class="gallery">
                    <img src="results/hall1.jpg" />
                    <img src="results/hall2.jpg" />
                   <img src="results/hall_combined.jpg" />
                </div>
  <h3 id="single-scale-alignment-results">TV Results</h3>
                <div class="gallery">
                    <img src="results/tv1.jpg" />
                    <img src="results/tv2.jpg" />
                   <img src="results/tv_combined.jpg" />
                </div>

  <h3 id="single-scale-alignment-results">Kitchen Results</h3>
                <div class="gallery">
                    <img src="results/kitchen1.jpg" />
                    <img src="results/kitchen2.jpg" />
                   <img src="results/kitchen_combined.jpg" />
                </div>


  <h1>CS 180 Project 4b: FEATURE MATCHING for AUTOSTITCHING</h1>

  <p>The goal of this project is to create a system for automatically stitching images into a mosaic. A secondary goal is to learn how to read and implement a research paper.</p>


  <h3>Step 1: Harris Interest Point Detection</h3>
  <p>
      To start, I implemented the Harris corner detector to identify interest points in a single-scale image, following Section 2 of the paper. Rather than developing a multi-scale approach or focusing on sub-pixel accuracy, I simplified the process by using a standard implementation of the Harris detector, provided in <code>harris.py</code>. This function calculates the Harris corner response across the image, highlighting areas with strong corner-like structures.
  </p>
  <p>
      Once the Harris corners were detected, I overlaid the corner points on the original image to visually represent where interest points were identified. This visualization helped verify that the corner detection was capturing key structural details in the image, which would later be essential for robust feature matching.
  </p>

  <h3>Step 2: Adaptive Non-Maximal Suppression (ANMS)</h3>
  <p>
      After detecting Harris corners, I implemented Adaptive Non-Maximal Suppression (ANMS) as described in Section 3 of the paper. ANMS refines the set of detected corners by prioritizing points that are not only strong in response but also well-distributed across the image. This step addresses a common issue in feature detection, where too many interest points cluster in certain regions, potentially leading to less effective matching.
  </p>
  <p>
      ANMS works by selecting a subset of corners based on a suppression radius that adapts according to corner strength. For each corner, I computed the distance to other stronger corners within the neighborhood and retained only those that had a high suppression radius. This ensured a uniform spatial distribution of points, improving the robustness of feature matching in areas of overlap between images. I visualized the final set of selected corners overlaid on the original image to confirm a balanced distribution.
  </p>

  <h3>Step 3: Feature Descriptor Extraction</h3>
  <p>
      With a refined set of interest points, I proceeded to extract feature descriptors from each selected corner, following Section 4 of the paper. Instead of implementing rotation-invariant descriptors, I simplified the approach by extracting axis-aligned, fixed-size patches, specifically 8x8 patches centered on each corner point. To capture enough context for each descriptor, I extracted each patch from a larger 40x40 window around each interest point.
  </p>
  <p>
      After sampling, each patch was bias/gain-normalized by subtracting the mean and dividing by the standard deviation. This normalization ensured that the descriptors were invariant to lighting variations, making the features more reliable across images. Each normalized patch was then flattened into a feature vector, which would later be used in the matching step.
  </p>

  <h3>Step 4: Feature Matching</h3>
  <p>
      The final step involved matching these feature descriptors between pairs of images. Following Section 5 of the paper and Lowe’s approach, I computed the sum of squared differences (SSD) between each pair of feature descriptors from the two images. For each feature in one image, I identified the nearest and second-nearest neighbors in the other image based on SSD distance. To filter out ambiguous matches, I applied Lowe’s ratio test, which compares the SSD ratio of the closest match to the second-closest match. Only matches with a ratio below a certain threshold were retained, ensuring that each matched pair was unique and distinctive.
  </p>
  <p>
      This matching process allowed me to identify pairs of points between images that had similar local structures. By visualizing the matches, I could confirm that they were accurate and meaningful, setting up a solid foundation for further steps, such as estimating a robust homography for image stitching.
  </p>


  <h2>Takeaways</h2>
  <p>The coolest thing I learned through this project was the power and versatility of feature-based image matching techniques. Diving into the paper gave me a deeper appreciation for the nuances of computer vision, especially in how robust matching pipelines can be created by carefully balancing detection, descriptor extraction, and spatial distribution of features. The implementation of Adaptive Non-Maximal Suppression (ANMS) was particularly enlightening; I realized how essential it is to ensure a uniform spatial distribution of features to improve matching accuracy and avoid clusters that might introduce ambiguity.
  </p>

  <p>Overall, it was a great learnings experience and a fun project.</p>
  

</body>
  
</html>
